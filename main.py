#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‹é“¾RSSè®¢é˜…èšåˆç¨‹åº
ä»å‹é“¾é¡µé¢å’Œæ‰‹åŠ¨é…ç½®åˆ—è¡¨ä¸­è·å–RSSæºï¼Œèšåˆæˆdata.json
"""

import os
import json
import re
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urljoin, urlparse
import hashlib
from time import sleep

import requests
import yaml
from bs4 import BeautifulSoup
import feedparser
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
REQUEST_TIMEOUT = 10
REQUEST_RETRIES = 1  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œé¿å…è¿‡å¤šå°è¯•
RETRY_BACKOFF = 0.3
FEED_CHECK_TIMEOUT = 5  # Feed URLæ£€æŸ¥ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
CACHE_FILE = 'feed_cache.json'


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œå­˜å‚¨å·²å‘ç°çš„RSSæºå’Œæ–‡ç« ID"""
    
    def __init__(self, cache_file: str = CACHE_FILE):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> dict:
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                return self._init_cache()
        return self._init_cache()
    
    def _init_cache(self) -> dict:
        """åˆå§‹åŒ–ç¼“å­˜ç»“æ„"""
        return {
            'feed_urls': {},  # {site_url: feed_url}
            'article_ids': set(),  # å·²å¤„ç†çš„æ–‡ç« IDï¼ˆç”¨äºå»é‡ï¼‰
            'last_update': None
        }
    
    def save(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            # å°†setè½¬æ¢ä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            cache_to_save = self.cache.copy()
            cache_to_save['article_ids'] = list(self.cache.get('article_ids', []))
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_to_save, f, ensure_ascii=False, indent=2)
            logger.debug(f"ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def get_cached_feed_url(self, site_url: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„Feed URL"""
        return self.cache.get('feed_urls', {}).get(site_url)
    
    def set_feed_url(self, site_url: str, feed_url: str):
        """ç¼“å­˜Feed URL"""
        if 'feed_urls' not in self.cache:
            self.cache['feed_urls'] = {}
        self.cache['feed_urls'][site_url] = feed_url
    
    def get_article_id(self, article: dict) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ID"""
        # ä½¿ç”¨æ ‡é¢˜å’Œå‘å¸ƒæ—¥æœŸçš„ç»„åˆä½œä¸ºID
        key = f"{article.get('title', '')}{article.get('pub_date', '')}".encode('utf-8')
        return hashlib.md5(key).hexdigest()
    
    def is_article_seen(self, article: dict) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å¤„ç†è¿‡"""
        if not isinstance(self.cache.get('article_ids'), set):
            self.cache['article_ids'] = set()
        article_id = self.get_article_id(article)
        return article_id in self.cache['article_ids']
    
    def mark_article_seen(self, article: dict):
        """æ ‡è®°æ–‡ç« ä¸ºå·²å¤„ç†"""
        if not isinstance(self.cache.get('article_ids'), set):
            self.cache['article_ids'] = set()
        article_id = self.get_article_id(article)
        self.cache['article_ids'].add(article_id)


class ConfigParser:
    """è§£æsetting.yamlé…ç½®æ–‡ä»¶"""
    
    def __init__(self, config_path: str = 'setting.yaml'):
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def get_link_pages(self) -> List[str]:
        """è·å–éœ€è¦çˆ¬å–çš„å‹é“¾é¡µé¢URLåˆ—è¡¨"""
        links = []
        for item in self.config.get('LINK', []):
            if isinstance(item, dict) and 'link' in item:
                links.append(item['link'])
        return links
    
    def get_link_page_rules(self) -> dict:
        """è·å–CSSé€‰æ‹©å™¨è§„åˆ™"""
        return self.config.get('link_page_rules', {})
    
    def get_block_sites(self) -> List[str]:
        """è·å–å±è”½ç«™ç‚¹åˆ—è¡¨"""
        return self.config.get('BLOCK_SITE', [])
    
    def get_block_site_reverse(self) -> bool:
        """è·å–æ˜¯å¦ä½¿ç”¨ç™½åå•æ¨¡å¼"""
        return self.config.get('BLOCK_SITE_REVERSE', False)
    
    def get_manual_links(self) -> List[Dict[str, str]]:
        """è·å–æ‰‹åŠ¨æ·»åŠ çš„å‹é“¾åˆ—è¡¨"""
        manual_links = []
        links_list = self.config.get('SETTINGS_FRIENDS_LINKS', {}).get('list', [])
        
        for item in links_list:
            if isinstance(item, list) and len(item) >= 3:
                link_dict = {
                    'name': item[0],
                    'url': item[1],
                    'avatar': item[2],
                    'feed_suffix': item[3] if len(item) > 3 else None
                }
                manual_links.append(link_dict)
        return manual_links
    
    def get_feed_suffixes(self) -> List[str]:
        """è·å–Feedåç¼€åˆ—è¡¨"""
        return self.config.get('feed_suffix', [])
    
    def get_max_posts(self) -> int:
        """è·å–æ¯ä¸ªç«™ç‚¹æœ€å¤šæŠ“å–æ–‡ç« æ•°"""
        return self.config.get('MAX_POSTS_NUM', 5)
    
    def get_outdate_days(self) -> int:
        """è·å–è¿‡æœŸæ–‡ç« å¤©æ•°"""
        return self.config.get('OUTDATE_CLEAN', 180)


class SiteFilter:
    """ç«™ç‚¹è¿‡æ»¤å™¨ï¼Œå¤„ç†é»‘/ç™½åå•"""
    
    def __init__(self, block_sites: List[str], reverse: bool = False):
        self.block_sites = block_sites
        self.reverse = reverse
    
    def is_blocked(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦è¢«å±è”½
        
        é»‘åå•æ¨¡å¼ (reverse=False): åŒ¹é…çš„è¢«å±è”½ï¼Œå…¶ä»–å…è®¸
        ç™½åå•æ¨¡å¼ (reverse=True): åŒ¹é…çš„è¢«å…è®¸ï¼Œå…¶ä»–å±è”½
        """
        for pattern in self.block_sites:
            if re.search(pattern, url):
                # åŒ¹é…åˆ°è§„åˆ™
                # é»‘åå•æ¨¡å¼: åŒ¹é…çš„è¢«å±è”½
                if not self.reverse:
                    return True
                # ç™½åå•æ¨¡å¼: åŒ¹é…çš„è¢«å…è®¸
                else:
                    return False
        
        # æœªåŒ¹é…åˆ°è§„åˆ™
        # é»‘åå•æ¨¡å¼: æœªåŒ¹é…çš„å…è®¸
        if not self.reverse:
            return False
        # ç™½åå•æ¨¡å¼: æœªåŒ¹é…çš„å±è”½
        else:
            return True


class LinkPageScraper:
    """å‹é“¾é¡µé¢çˆ¬è™«"""
    
    def __init__(self, rules: dict):
        self.rules = rules
        self.session = self._create_session()
    
    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requestsä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥ï¼šåªé‡è¯•1æ¬¡ï¼Œå¿«é€Ÿå¤±è´¥
        retry_strategy = Retry(
            total=REQUEST_RETRIES,
            backoff_factor=RETRY_BACKOFF,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"],
            raise_on_status=False  # ä¸è¦åœ¨çŠ¶æ€é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({'User-Agent': USER_AGENT})
        session.verify = False  # ç¦ç”¨SSLéªŒè¯ï¼Œé¿å…è‡ªç­¾åè¯ä¹¦é”™è¯¯
        return session
    
    def scrape(self, url: str) -> List[Dict[str, str]]:
        """ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥"""
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–å‹é“¾é¡µé¢: {url}")
            response = self.session.get(url, timeout=REQUEST_TIMEOUT)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = []
            author_elements = soup.select(self.rules.get('author', [{}])[0].get('selector', ''))
            
            for author_elem in author_elements:
                try:
                    # æŸ¥æ‰¾è¯¥ä½œè€…å…ƒç´ å¯¹åº”çš„é“¾æ¥
                    link_elem = author_elem.find_parent().find('a') if author_elem.find_parent() else author_elem
                    if not link_elem:
                        link_elem = author_elem
                    
                    link_url = link_elem.get('href') or link_elem.get('data-href', '')
                    author_name = author_elem.get_text(strip=True) or link_elem.get_text(strip=True)
                    
                    # å°è¯•è·å–å¤´åƒ
                    avatar = ''
                    img_elem = author_elem.find_parent().find('img') if author_elem.find_parent() else None
                    if not img_elem:
                        img_elem = author_elem.find('img')
                    if img_elem:
                        avatar = img_elem.get('src', '')
                    
                    if link_url and author_name:
                        # è§„èŒƒåŒ–URL
                        if not link_url.startswith('http'):
                            link_url = urljoin(url, link_url)
                        
                        links.append({
                            'name': author_name,
                            'url': link_url,
                            'avatar': avatar
                        })
                except Exception as e:
                    logger.debug(f"çˆ¬å–å•æ¡é“¾æ¥å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ä»{url}æˆåŠŸçˆ¬å–{len(links)}æ¡é“¾æ¥")
            return links
        except requests.Timeout:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢è¶…æ—¶ {url}")
            return []
        except requests.HTTPError as e:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢HTTPé”™è¯¯ {url}: {e.response.status_code}")
            return []
        except Exception as e:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢å¤±è´¥ {url}: {e}")
            return []


class RSSFetcher:
    """RSSæºè·å–å™¨"""
    
    def __init__(self, feed_suffixes: List[str], max_posts: int, cache_manager: Optional['CacheManager'] = None):
        self.feed_suffixes = feed_suffixes
        self.max_posts = max_posts
        self.session = self._create_session()
        self.check_session = self._create_check_session()  # ç”¨äºå¿«é€Ÿæ£€æŸ¥Feed URLçš„ä¼šè¯
        self.cache = cache_manager
        # æœ€è¿‘ä¸€æ¬¡è·å–/è§£æ RSS æ—¶çš„é”™è¯¯ä¿¡æ¯ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¾›å¤–éƒ¨æŸ¥è¯¢
        self.last_error: Optional[str] = None
    
    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requestsä¼šè¯ï¼ˆç”¨äºè·å–RSSå†…å®¹ï¼‰"""
        session = requests.Session()
        
        retry_strategy = Retry(
            total=REQUEST_RETRIES,
            backoff_factor=RETRY_BACKOFF,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({'User-Agent': USER_AGENT})
        session.verify = False
        return session
    
    def _create_check_session(self) -> requests.Session:
        """åˆ›å»ºä¸è¿›è¡Œé‡è¯•çš„ä¼šè¯ï¼ˆç”¨äºå¿«é€Ÿæ£€æŸ¥Feed URLï¼‰"""
        session = requests.Session()
        
        # ä¸è¿›è¡Œä»»ä½•é‡è¯•ï¼Œå¿«é€Ÿå¤±è´¥
        adapter = HTTPAdapter(max_retries=0)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({'User-Agent': USER_AGENT})
        session.verify = False
        return session
    
    def find_feed_url(self, base_url: str, custom_suffix: Optional[str] = None) -> Optional[str]:
        """å¯»æ‰¾ç«™ç‚¹çš„RSSæºURL
        
        ä¼˜å…ˆçº§ï¼š
        1. æ£€æŸ¥ç¼“å­˜
        2. å°è¯•è‡ªå®šä¹‰åç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        3. å°è¯•å¸¸è§Feedåç¼€ï¼ˆå¿«é€Ÿå¤±è´¥ï¼‰
        """
        # å…ˆæ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached_url = self.cache.get_cached_feed_url(base_url)
            if cached_url:
                if self._check_feed_url(cached_url):
                    logger.debug(f"âœ“ ä½¿ç”¨ç¼“å­˜çš„Feed: {cached_url}")
                    return cached_url
        
        # ç¡®ä¿base_urlä»¥/ç»“å°¾
        base_url_normalized = base_url if base_url.endswith('/') else base_url + '/'
        
        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰åç¼€ï¼Œé¦–å…ˆå°è¯•
        if custom_suffix:
            feed_url = urljoin(base_url_normalized, custom_suffix)
            if self._check_feed_url(feed_url):
                if self.cache:
                    self.cache.set_feed_url(base_url.rstrip('/'), feed_url)
                return feed_url
        
        # å°è¯•å¸¸è§çš„Feedåç¼€
        for suffix in self.feed_suffixes:
            feed_url = urljoin(base_url_normalized, suffix)
            if self._check_feed_url(feed_url):
                if self.cache:
                    self.cache.set_feed_url(base_url.rstrip('/'), feed_url)
                return feed_url
        
        return None
    
    def _check_feed_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„Feedæºï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼Œä¸é‡è¯•ï¼‰"""
        try:
            # ä½¿ç”¨ä¸é‡è¯•çš„ä¼šè¯å’Œæ›´çŸ­çš„è¶…æ—¶
            response = self.check_session.get(url, timeout=FEED_CHECK_TIMEOUT)
            
            if response.status_code != 200:
                self.last_error = f"HTTP {response.status_code}"
                logger.debug(f"Feed URLæ£€æŸ¥å¤±è´¥ {url} (HTTP {response.status_code})")
                return False
            
            content_type = response.headers.get('content-type', '').lower()
            text_lower = response.text[:500].lower()  # åªæ£€æŸ¥å‰500å­—ç¬¦
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„XML/RSS/Atomæº
            is_feed = ('xml' in content_type or 'rss' in content_type or 'feed' in content_type or
                      '<?xml' in text_lower or '<rss' in text_lower or '<feed' in text_lower)
            
            if is_feed:
                logger.debug(f"âœ“ æ‰¾åˆ°æœ‰æ•ˆFeedæº: {url}")
            else:
                self.last_error = "not_feed_format"
                logger.debug(f"âœ— URLä¸æ˜¯Feedæ ¼å¼: {url}")

            return is_feed
                
        except requests.Timeout:
            logger.debug(f"Feed URLæ£€æŸ¥è¶…æ—¶: {url}")
            return False
        except requests.ConnectionError:
            logger.debug(f"Feed URLè¿æ¥å¤±è´¥: {url}")
            return False
        except Exception as e:
            logger.debug(f"Feed URLæ£€æŸ¥å¼‚å¸¸ {url}: {type(e).__name__}")
            return False
    
    def fetch_feed(self, feed_url: str) -> Optional[feedparser.FeedParserDict]:
        """è·å–å’Œè§£æRSSæº"""
        try:
            logger.info(f"æ­£åœ¨è·å–RSSæº: {feed_url}")
            
            # ä½¿ç”¨requestsè·å–å†…å®¹ï¼Œç„¶åä¼ ç»™feedparser
            response = self.session.get(feed_url, timeout=REQUEST_TIMEOUT)
            
            if response.status_code != 200:
                self.last_error = f"HTTP {response.status_code}"
                logger.warning(f"è·å–RSSæºå¤±è´¥ï¼ŒHTTP {response.status_code}: {feed_url}")
                return None
            
            feed = feedparser.parse(response.content)
            
            if feed.bozo and isinstance(feed.bozo_exception, Exception):
                self.last_error = str(feed.bozo_exception)
                logger.debug(f"RSSè§£æå¼‚å¸¸ {feed_url}: {feed.bozo_exception}")
            
            if not feed.entries:
                # æ— æ¡ç›®è§†ä¸ºè§£æ/å†…å®¹é—®é¢˜
                self.last_error = "empty_or_unparseable"
                logger.warning(f"RSSæºä¸ºç©ºæˆ–æ— æ³•è§£æ: {feed_url}")
                return None
            
            return feed
        except requests.Timeout:
            self.last_error = 'timeout'
            logger.warning(f"è·å–RSSæºè¶…æ—¶: {feed_url}")
            return None
        except requests.ConnectionError as e:
            self.last_error = type(e).__name__
            logger.warning(f"è·å–RSSæºè¿æ¥é”™è¯¯ {feed_url}: {type(e).__name__}")
            return None
        except requests.HTTPError as e:
            self.last_error = f"HTTPError {e.response.status_code}"
            logger.warning(f"è·å–RSSæºHTTPé”™è¯¯ {feed_url}: {e.response.status_code}")
            return None
        except Exception as e:
            self.last_error = str(e)
            logger.warning(f"è·å–RSSæºå¤±è´¥ {feed_url}: {type(e).__name__}")
            return None


class DataAggregator:
    """æ•°æ®èšåˆå™¨"""
    
    def __init__(self, max_posts: int, outdate_days: int):
        self.max_posts = max_posts
        self.outdate_days = outdate_days
        # å¦‚æœ outdate_days <= 0 åˆ™è¡¨ç¤ºä¸é™åˆ¶è¿‡æœŸï¼Œcutoff_time è®¾ä¸º None
        if outdate_days and outdate_days > 0:
            self.cutoff_time = datetime.now() - timedelta(days=outdate_days)
        else:
            self.cutoff_time = None
    
    def aggregate_feed(self, site_info: Dict[str, str], feed: feedparser.FeedParserDict) -> Dict[str, Any]:
        """èšåˆå•ä¸ªç«™ç‚¹çš„Feedæ•°æ®"""
        site_data = {
            'name': site_info['name'],
            'url': site_info['url'],
            'avatar': site_info['avatar'],
            'feed_url': site_info.get('feed_url', ''),
            'posts': []
        }
        
        # æå–Feedä¿¡æ¯
        feed_title = feed.feed.get('title', site_info['name'])
        
        posts = []
        for entry in feed.entries:
            try:
                # è·å–å‘å¸ƒæ—¶é—´
                pub_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_time = datetime(*entry.updated_parsed[:6])
                else:
                    pub_time = datetime.now()
                
                # è¿‡æ»¤è¿‡æœŸæ–‡ç« ï¼ˆå½“è®¾ç½®ä¸º0æˆ–è´Ÿæ•°æ—¶è¡¨ç¤ºä¸é™åˆ¶ï¼‰
                if self.cutoff_time is not None and pub_time < self.cutoff_time:
                    continue
                
                # è·å–æ›´æ–°æ—¶é—´ï¼ˆä¼˜å…ˆä½¿ç”¨updated_parsedï¼Œå¦åˆ™ä½¿ç”¨pub_timeï¼‰
                update_time = None
                if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    update_time = datetime(*entry.updated_parsed[:6])
                else:
                    update_time = pub_time
                
                post = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'link': entry.get('link', ''),
                    'description': entry.get('summary', ''),
                    'pub_date': pub_time.isoformat(),
                    'updated_at': update_time.isoformat(),
                    'author': entry.get('author', '')
                }
                posts.append(post)
            except Exception as e:
                logger.debug(f"å¤„ç†Feedæ¡ç›®å¤±è´¥: {e}")
                continue
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºå¹¶é™åˆ¶æ•°é‡
        posts.sort(key=lambda x: x['pub_date'], reverse=True)
        site_data['posts'] = posts[:self.max_posts] if self.max_posts > 0 else posts
        
        return site_data
    
    def merge_data(self, all_sites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰ç«™ç‚¹æ•°æ®"""
        all_posts = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« 
        for site in all_sites:
            for post in site['posts']:
                post['site_name'] = site['name']
                post['site_url'] = site['url']
                post['avatar'] = site['avatar']
                all_posts.append(post)
        
        # æŒ‰æ—¶é—´æ’åº
        all_posts.sort(key=lambda x: x['pub_date'], reverse=True)
        
        return {
            'updated_at': datetime.now().isoformat(),
            'total_sites': len(all_sites),
            'total_posts': len(all_posts),
            'sites': all_sites,
            'all_posts': all_posts
        }


class FriendRSSAggregator:
    """ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config_path: str = 'setting.yaml'):
        self.config = ConfigParser(config_path)
        self.cache = CacheManager()
        self.site_filter = SiteFilter(
            self.config.get_block_sites(),
            self.config.get_block_site_reverse()
        )
        self.scraper = LinkPageScraper(self.config.get_link_page_rules())
        self.fetcher = RSSFetcher(
            self.config.get_feed_suffixes(),
            self.config.get_max_posts(),
            self.cache
        )
        self.aggregator = DataAggregator(
            self.config.get_max_posts(),
            self.config.get_outdate_days()
        )
        # ç”¨äºè®°å½•è·å– RSS å¤±è´¥çš„ç«™ç‚¹åˆ—è¡¨
        self.failed_sites: List[Dict[str, Any]] = []
    
    def get_all_links(self) -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰å‹é“¾
        
        å¤„ç†é¡ºåºï¼š
        1. ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥
        2. å¯¹çˆ¬å–çš„é“¾æ¥è¿›è¡Œå±è”½æ£€æŸ¥
        3. å°è¯•è·å–RSSæºå¹¶ç¼“å­˜
        4. æ·»åŠ æ‰‹åŠ¨é…ç½®çš„é“¾æ¥
        """
        all_links = []
        url_set = set()
        
        # ã€ç¬¬ä¸€æ­¥ã€‘ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥ï¼Œå¹¶å°è¯•å‘ç°RSSæº
        logger.info("ã€ç¬¬ä¸€æ­¥ã€‘çˆ¬å–å‹é“¾é¡µé¢å¹¶å‘ç°RSSæº...")
        for page_url in self.config.get_link_pages():
            scraped_links = self.scraper.scrape(page_url)
            for link in scraped_links:
                # æ£€æŸ¥æ˜¯å¦è¢«å±è”½
                if self.site_filter.is_blocked(link['url']):
                    logger.debug(f"çˆ¬å–å‹é“¾è¢«å±è”½: {link['name']} ({link['url']})")
                    continue
                
                # å»é‡
                if link['url'] in url_set:
                    logger.debug(f"å‹é“¾å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤: {link['name']}")
                    continue
                
                # å°è¯•å‘ç°RSSæº
                feed_url = self.fetcher.find_feed_url(link['url'])
                if feed_url:
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²å‘ç°RSSæº: {link['name']} -> {feed_url}")
                else:
                    logger.debug(f"æœªæ‰¾åˆ°RSSæº: {link['name']}")
                
                all_links.append(link)
                url_set.add(link['url'])
                logger.debug(f"æ·»åŠ çˆ¬å–å‹é“¾: {link['name']} ({link['url']})")
        
        # ã€ç¬¬äºŒæ­¥ã€‘æ·»åŠ æ‰‹åŠ¨é…ç½®çš„é“¾æ¥
        logger.info("ã€ç¬¬äºŒæ­¥ã€‘æ·»åŠ æ‰‹åŠ¨é…ç½®çš„å‹é“¾...")
        manual_links = self.config.get_manual_links()
        for link in manual_links:
            # æ£€æŸ¥æ˜¯å¦è¢«å±è”½
            if self.site_filter.is_blocked(link['url']):
                logger.debug(f"æ‰‹åŠ¨å‹é“¾è¢«å±è”½: {link['name']} ({link['url']})")
                continue
            
            # å»é‡
            if link['url'] in url_set:
                logger.debug(f"æ‰‹åŠ¨å‹é“¾å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤: {link['name']}")
                continue
            
            # å¦‚æœæœ‰è‡ªå®šä¹‰Feedåç¼€ï¼ŒæŒ‰ç”¨æˆ·é€‰æ‹© Aï¼šè·³è¿‡å¿«é€Ÿæ£€æŸ¥ï¼Œç›´æ¥æ‹¼æ¥å¹¶è®¾ç½®ä¸º feed_urlï¼ˆfetch é˜¶æ®µä»ä¼šå°è¯•è§£æï¼‰
            if link.get('feed_suffix'):
                try:
                    base = link['url'] if link['url'].endswith('/') else link['url'] + '/'
                    feed_url = urljoin(base, link['feed_suffix'])
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²è®¾ç½®è‡ªå®šä¹‰RSSæºï¼ˆè·³è¿‡æ£€æŸ¥ï¼‰: {link['name']} -> {feed_url}")
                except Exception:
                    logger.debug(f"æ„å»ºè‡ªå®šä¹‰RSSæºå¤±è´¥: {link['name']} ({link.get('url')})")
            else:
                feed_url = self.fetcher.find_feed_url(link['url'])
                if feed_url:
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²å‘ç°RSSæº: {link['name']} -> {feed_url}")
            
            all_links.append(link)
            url_set.add(link['url'])
            logger.debug(f"æ·»åŠ æ‰‹åŠ¨å‹é“¾: {link['name']} ({link['url']})")
        
        logger.info(f"å…±è·å–{len(all_links)}æ¡æœ‰æ•ˆå‹é“¾")
        return all_links
    
    def process_site(self, link: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªç«™ç‚¹ï¼Œè·å–å…¶RSSæ•°æ®"""
        try:
            # å¦‚æœä¹‹å‰å·²ç»å‘ç°äº†Feed URLï¼Œç›´æ¥ä½¿ç”¨
            feed_url = link.get('feed_url')
            
            if not feed_url:
                # å¦‚æœæ²¡æœ‰é¢„å…ˆå‘ç°ï¼Œå†æ¬¡å°è¯•å¯»æ‰¾ï¼ˆå¤‡ç”¨ï¼‰
                feed_url = self.fetcher.find_feed_url(
                    link['url'],
                    link.get('feed_suffix')
                )
            
            if not feed_url:
                logger.warning(f"æ— æ³•æ‰¾åˆ°{link['name']}çš„RSSæº: {link['url']}")
                # è®°å½•å¤±è´¥ç«™ç‚¹
                self.failed_sites.append({
                    'name': link.get('name'),
                    'url': link.get('url'),
                    'feed_url': None,
                    'reason': 'no_feed_found'
                })
                return None
            
            # è·å–Feed
            feed = self.fetcher.fetch_feed(feed_url)
            if not feed:
                # è®°å½• fetch å¤±è´¥åŠå…¶åŸå› ï¼ˆfetcher.last_errorï¼‰
                self.failed_sites.append({
                    'name': link.get('name'),
                    'url': link.get('url'),
                    'feed_url': feed_url,
                    'reason': getattr(self.fetcher, 'last_error', 'fetch_failed')
                })
                return None
            
            site_info = {
                'name': link['name'],
                'url': link['url'],
                'avatar': link.get('avatar', ''),
                'feed_url': feed_url
            }
            
            site_data = self.aggregator.aggregate_feed(site_info, feed)
            logger.info(f"æˆåŠŸå¤„ç†{link['name']}: è·å–{len(site_data['posts'])}ç¯‡æ–‡ç« ")
            return site_data
        
        except Exception as e:
            logger.error(f"å¤„ç†ç«™ç‚¹{link.get('name', link['url'])}å¤±è´¥: {e}")
            self.failed_sites.append({
                'name': link.get('name'),
                'url': link.get('url'),
                'feed_url': link.get('feed_url'),
                'reason': str(e)
            })
            return None
    
    def run(self) -> dict:
        """æ‰§è¡Œä¸»æµç¨‹"""
        logger.info("=" * 50)
        logger.info("å¼€å§‹å‹é“¾RSSèšåˆ")
        logger.info("=" * 50)
        
        # è·å–æ‰€æœ‰é“¾æ¥
        all_links = self.get_all_links()
        
        # å¤„ç†æ¯ä¸ªç«™ç‚¹
        all_sites = []
        for link in all_links:
            site_data = self.process_site(link)
            if site_data:
                all_sites.append(site_data)
        
        # åˆå¹¶æ•°æ®
        final_data = self.aggregator.merge_data(all_sites)
        # æŠŠå¤±è´¥ç«™ç‚¹ä¿¡æ¯æ”¾å…¥æœ€ç»ˆç»“æœ
        final_data['failed_sites'] = self.failed_sites
        
        logger.info("=" * 50)
        logger.info(f"èšåˆå®Œæˆ: {final_data['total_sites']}ä¸ªç«™ç‚¹, {final_data['total_posts']}ç¯‡æ–‡ç« ")
        logger.info("=" * 50)
        
        # ä¿å­˜ç¼“å­˜
        self.cache.save()
        
        return final_data
    
    def save_to_file(self, data: dict, output_path: str = 'data.json'):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°{output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        aggregator = FriendRSSAggregator('setting.yaml')
        data = aggregator.run()
        aggregator.save_to_file(data, 'data.json')
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  âœ“ æ€»ç«™ç‚¹æ•°: {data['total_sites']}")
        logger.info(f"  âœ“ æ€»æ–‡ç« æ•°: {data['total_posts']}")
        logger.info(f"  âœ“ æ›´æ–°æ—¶é—´: {data['updated_at']}")
        logger.info("âœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    main()
